# *Everything You Ever Wanted to Know About Text Tokenization* (But Were Afraid to Ask)

* **Writers:** Thaís Martins, Thainara Assis, Gustavo Bernardo</font>

* **Reviewer:** João Gabriel Matos, Thaís Martins 

Welcome to our **Tokenization Research Project**, developed by the **TELUS Digital Research Hub Undergraduate Students**. 

This repository hosts our comprehensive study on text tokenization methods, covering **word-, character-, and subword-level** algorithms such as BPE, WordPiece, and Unigram, and extends to discussions on **multilingual, mathematical, and code tokenization.** It examines their efficiency, consistency, semantic preservation, and influence on large language models.

## ***Summary***
1. Introduction
2. **Word-Level** Tokenization: The most intuitive alternative
3. **Character-Level** Tokenization: Breaking it all the way down
4. **Subword-Level** Tokenization: The sweet spot of modern NLP
5. Why Models Struggle With Letters and Spelling
6. Tokenization of **Different Languages**
7. Tokenization of **Mathematics**
8. Tokenization of **Programming Languages**
9. The Impact of Tokenization and Vectorization
